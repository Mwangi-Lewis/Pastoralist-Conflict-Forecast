# -*- coding: utf-8 -*-
"""West Pokot 2015-2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yOHMQKRiPWGiuFBf6EEvSqycick7N1io

Step 1:Import the libraries
"""

!pip -q install geopandas pyogrio shapely xgboost

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score

"""Step 2: Load the dataset"""

COUNTY      = "West Pokot"
NDVI_CSV    = "/content/drive/MyDrive/westpokot_ndvi_pointgrid_2015_2024.csv"
RAIN_CSV    = "/content/drive/MyDrive/westpokot_chirps16d_2015_2024.csv"
POINTS_GEO  = "/content/drive/MyDrive/westpokot_grid_points_5km.geojson"

# Where to write outputs (change if you want Drive)
OUT_DIR     = "/content"

OUT_DIR = Path(OUT_DIR)
OUT_DIR.mkdir(parents=True, exist_ok=True)

ndvi = pd.read_csv(NDVI_CSV)
rain = pd.read_csv(RAIN_CSV)

# Normalize column names expected by later code
# (GEE exports from the script have: cell_id, date, ndvi_mean / rain_16d)
ndvi.rename(columns={'NDVI':'ndvi_mean','ndvi':'ndvi_mean'}, inplace=True)
rain.rename(columns={'precipitation':'rain_16d'}, inplace=True)

# Parse dates
ndvi['date'] = pd.to_datetime(ndvi['date'])
rain['date'] = pd.to_datetime(rain['date'])

# Keep only necessary columns
ndvi = ndvi[['cell_id','date', 'ndvi_mean']].dropna()
rain = rain[['cell_id','date', 'rain_16d']].dropna()

# Merge
df = pd.merge_asof(
        ndvi, rain,
        by='cell_id',
        on='date',
        direction='nearest',
        tolerance=pd.Timedelta('8D')
     ).sort_values(['cell_id','date']).reset_index(drop=True)

# Basic cleaning
df['ndvi'] = df['ndvi_mean'].astype(float)
df['rain_16d'] = df['rain_16d'].astype(float)
df.drop(columns=['ndvi_mean'], inplace=True)

"""3) Feature engineering (lags + z-scores + label)"""

# Z-score of NDVI per cell (anomaly vs that cell’s history)
# ddof=0 makes it robust when series is short
df['ndvi_mean_cell'] = df.groupby('cell_id')['ndvi'].transform('mean')
df['ndvi_std_cell']  = df.groupby('cell_id')['ndvi'].transform(lambda s: s.std(ddof=0))
df['ndvi_z'] = (df['ndvi'] - df['ndvi_mean_cell']) / (df['ndvi_std_cell'].replace(0, np.nan))

# Lags (per cell)
for col in ['ndvi','rain_16d','ndvi_z']:
    df[f'{col}_lag1'] = df.groupby('cell_id')[col].shift(1)
    df[f'{col}_lag2'] = df.groupby('cell_id')[col].shift(2)

# Next-period z-score and binary label (hotspot if vegetation stress likely next)
df['ndvi_z_next'] = df.groupby('cell_id')['ndvi_z'].shift(-1)

# ---- primary label (can relax if too sparse)
LABEL_Z = -0.8    # <- use -0.8 (less strict than -1.0)
df['hotspot_next'] = (df['ndvi_z_next'] < LABEL_Z).astype('int8')

# Keep rows that have all features available
feat_cols = [
    'ndvi','rain_16d','ndvi_z',
    'ndvi_lag1','ndvi_lag2',
    'rain_16d_lag1','rain_16d_lag2',
    'ndvi_z_lag1','ndvi_z_lag2'
]
needed = feat_cols + ['hotspot_next']
Xy = df.dropna(subset=needed).copy()

print("Overall class balance:")
print(Xy['hotspot_next'].value_counts(dropna=False))

"""4) Robust time split + baseline model (auto-heals single-class splits)"""

def has_two_classes(y: pd.Series) -> bool:
    return y.nunique() >= 2

def precision_at_k(y_true, y_score, k=0.10):
    n = max(1, int(len(y_score) * k))
    idx = np.argsort(-y_score)[:n]
    pred = np.zeros_like(y_true); pred[idx] = 1
    return precision_score(y_true, pred, zero_division=0)

# initial split target
split_date = pd.Timestamp('2021-01-01')

def make_split_with_two_classes(Xy, split_date):
    train = Xy[Xy['date'] < split_date]
    test  = Xy[Xy['date'] >= split_date]
    if has_two_classes(train['hotspot_next']) and has_two_classes(test['hotspot_next']):
        return train, test, split_date

    # try several quantile-based dates
    for q in [0.6, 0.7, 0.75, 0.8, 0.85]:
        sd = pd.to_datetime(Xy['date'].quantile(q))
        train = Xy[Xy['date'] < sd]
        test  = Xy[Xy['date'] >= sd]
        if has_two_classes(train['hotspot_next']) and has_two_classes(test['hotspot_next']):
            print(f"[INFO] Adjusted split_date to {sd.date()} (q={q}) to get both classes.")
            return train, test, sd
    return None, None, None

train, test, used_split = make_split_with_two_classes(Xy, split_date)

# fallback: if still single-class, use percentile label to ensure positives exist
if train is None:
    print("[WARN] Could not find a two-class split. Falling back to percentile label (bottom 20%).")
    p20 = df['ndvi_z_next'].quantile(0.20)
    df['hotspot_next'] = (df['ndvi_z_next'] <= p20).astype('int8')
    Xy = df.dropna(subset=needed).copy()
    train, test, used_split = make_split_with_two_classes(Xy, Xy['date'].quantile(0.8))

print("Split used:", used_split)
print("Train class counts:", train['hotspot_next'].value_counts().to_dict())
print("Test  class counts:", test['hotspot_next'].value_counts().to_dict())

X_train = train[feat_cols].values
y_train = train['hotspot_next'].values
X_test  = test[feat_cols].values
y_test  = test['hotspot_next'].values

clf = LogisticRegression(max_iter=300, class_weight='balanced')
clf.fit(X_train, y_train)
proba = clf.predict_proba(X_test)[:, 1]

roc = roc_auc_score(y_test, proba)
pr  = average_precision_score(y_test, proba)
p10 = precision_at_k(y_test, proba, 0.10)
print(f"[{COUNTY}] Logistic Regression  ROC-AUC={roc:.3f}  PR-AUC={pr:.3f}  Precision@10%={p10:.3f}")

"""5) Score last train date & export map files"""

# Score on the latest date before the split (t), predicting t+1 risk
score_date = df[df['date'] < used_split]['date'].max()
print("Scoring date (t):", score_date)

# Ensure lags exist (safe if already done)
for col in ['ndvi','rain_16d','ndvi_z']:
    for k in [1,2]:
        key = f'{col}_lag{k}'
        if key not in df.columns:
            df[key] = df.groupby('cell_id')[col].shift(k)

score = df[df['date'] == score_date].dropna(subset=feat_cols).copy()

if score.empty:
    print(f"No data available for scoring on {score_date}. The 'score' DataFrame is empty after dropping rows with missing features.")
else:
    score['p_hotspot_next'] = clf.predict_proba(score[feat_cols].values)[:, 1]
    score_out = score[['cell_id','date','p_hotspot_next']].reset_index(drop=True)

    print("Rows scored:", len(score_out))
    display(score_out.head())

"""Join with grid points & Save (GeoJSON + CSV + metrics)"""

points_gdf = gpd.read_file(POINTS_GEO)
if points_gdf.crs is None:
    points_gdf = points_gdf.set_crs(4326)
else:
    points_gdf = points_gdf.to_crs(4326)

# --- Helper to ensure a 'cell_id' exists in the GeoDataFrame
def ensure_cell_id_in_points(gdf):
    # Try common variants
    lower_map = {c.lower(): c for c in gdf.columns}
    candidates = [
        'cell_id', 'cellid', 'cell_id ', 'cell-id', 'cell_id_', 'cell id',
        'cellId', 'Cell_ID'
    ]
    for cand in candidates:
        if cand in lower_map:
            true_col = lower_map[cand]
            if true_col != 'cell_id':
                gdf = gdf.rename(columns={true_col: 'cell_id'})
            break

    # If still missing, rebuild from geometry (lon/lat rounded like your GEE ID)
    if 'cell_id' not in gdf.columns:
        # Ensure lon/lat columns exist by using representative points
        gdf['longitude'] = gdf.geometry.apply(lambda geom: geom.centroid.x if geom else None)
        gdf['latitude']  = gdf.geometry.apply(lambda geom: geom.centroid.y if geom else None)

        # Handle potential None values if centroid calculation failed for some geometries
        gdf = gdf.dropna(subset=['longitude', 'latitude'])

        gdf['cell_id'] = (
            np.round(gdf['longitude']*100).astype(int).astype(str) + '_' +
            np.round(gdf['latitude']*100).astype(int).astype(str)
        )
    return gdf


# Ensure cell_id exists in both frames and is clean string
points_gdf = ensure_cell_id_in_points(points_gdf)
if 'cell_id' not in score_out.columns:
    # Try to recover from common mistakes
    lower_map = {c.lower(): c for c in score_out.columns}
    if 'cell_id' in lower_map:
        score_out = score_out.rename(columns={lower_map['cell_id']: 'cell_id'})
    else:
        raise KeyError("score_out is missing 'cell_id' — check earlier steps building score_out.")

# Strip and cast to string to avoid type mis-match
points_gdf['cell_id'] = points_gdf['cell_id'].astype(str).str.strip()
score_out['cell_id']  = score_out['cell_id'].astype(str).str.strip()

# --- Now the merge will work
mapped = points_gdf.merge(score_out, on='cell_id', how='left').dropna(subset=['p_hotspot_next'])
print("Mapped rows:", len(mapped))

prefix       = "westpokot_hotspot_forecast"
geojson_out  = OUT_DIR / f"{prefix}.geojson"
csv_out      = OUT_DIR / f"{prefix}.csv"
metrics_out  = OUT_DIR / f"{COUNTY.lower().replace(' ','')}_model_metrics.txt"

mapped.to_file(geojson_out, driver='GeoJSON')
mapped[['cell_id','p_hotspot_next']].sort_values('p_hotspot_next', ascending=False).to_csv(csv_out, index=False)

with open(metrics_out, "w") as f:
    f.write(f"County: {COUNTY}\n")
    f.write(f"Training/Test split date: {pd.to_datetime(used_split).date()}\n")
    f.write(f"Scored period (t): {pd.to_datetime(score_date).date()}\n")
    f.write(f"ROC-AUC: {roc:.4f}\n")
    f.write(f"PR-AUC : {pr:.4f}\n")
    f.write(f"Precision@10%: {p10:.4f}\n")
    f.write(f"Train positives: {int(train['hotspot_next'].sum())}\n")
    f.write(f"Test positives : {int(test['hotspot_next'].sum())}\n")


print("Saved files:")
print(" -", geojson_out)
print(" -", csv_out)
print(" -", metrics_out)