# -*- coding: utf-8 -*-
"""Wajir 2015-2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M654sgjOWKQDtQNo14vQRbp42nO33Ryc

Step 1:Import libraries
"""

!pip -q install geopandas pyogrio shapely xgboost

import pandas as pd
import numpy as np
import geopandas as gpd
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score

"""Step 2:Load the dataset"""

from google.colab import drive
drive.mount('/content/drive')
DATA = Path('/content/drive/MyDrive')  # change if needed
ndvi_csv   = DATA / 'wajir_ndvi_pointgrid_2015_2024.csv'
rain_csv   = DATA / 'wajir_chirps16d_2015_2024.csv'
points_geo = DATA / 'wajir_grid_points_5km.geojson'
OUT_DIR    = Path('/content')           # where to save outputs

def read_ndvi(p):
    df = pd.read_csv(p)
    df = df.rename(columns={'ndvi_mean':'ndvi'})
    df['date'] = pd.to_datetime(df['date'])
    return df[['cell_id','date','ndvi']]

def read_rain(p):
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'])
    return df[['cell_id','date','rain_16d']]

ndvi = read_ndvi(ndvi_csv)
rain = read_rain(rain_csv)

df = (ndvi.merge(rain, on=['cell_id','date'], how='inner')
           .sort_values(['cell_id','date'])
           .reset_index(drop=True))
print(df.shape, df.head())

"""Step 3:NDVI anomaly + lags + label"""

# Fortnight index
df['fortnight'] = ((df['date'].dt.dayofyear - 1)//16).astype(int)

# Climatology per cell & fortnight
grp = df.groupby(['cell_id','fortnight'])
mu   = grp['ndvi'].transform('mean')
sd   = grp['ndvi'].transform('std').replace(0, np.nan)
df['ndvi_z'] = (df['ndvi'] - mu) / sd

# Lags
for col in ['ndvi','rain_16d','ndvi_z']:
    df[f'{col}_lag1'] = df.groupby('cell_id')[col].shift(1)
    df[f'{col}_lag2'] = df.groupby('cell_id')[col].shift(2)

# Target: next-period stress
df['ndvi_z_next']   = df.groupby('cell_id')['ndvi_z'].shift(-1)
df['hotspot_next']  = (df['ndvi_z_next'] < -1.0).astype('int8')

feat_cols = [
    'ndvi','rain_16d','ndvi_z',
    'ndvi_lag1','ndvi_lag2',
    'rain_16d_lag1','rain_16d_lag2',
    'ndvi_z_lag1','ndvi_z_lag2'
]
Xy = df.dropna(subset=feat_cols+['hotspot_next']).copy()
print("Trainable rows:", Xy.shape[0])
print(Xy['hotspot_next'].value_counts(normalize=True))

"""Step 4:Train/test (time split) + baseline model"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score

def has_two_classes(y: pd.Series) -> bool:
    return y.nunique() >= 2

def precision_at_k(y_true, y_score, k=0.10):
    n = max(1, int(len(y_score) * k))
    idx = np.argsort(-y_score)[:n]
    pred = np.zeros_like(y_true); pred[idx] = 1
    return precision_score(y_true, pred, zero_division=0)

# ---- initial split you wanted
split_date = pd.Timestamp('2021-01-01')   # changeable

# Try to ensure both train and test have 0/1
def make_split_with_two_classes(Xy, split_date):
    train = Xy[Xy['date'] < split_date]
    test  = Xy[Xy['date'] >= split_date]
    if has_two_classes(train['hotspot_next']) and has_two_classes(test['hotspot_next']):
        return train, test, split_date

    # Try a few quantile-based split dates
    for q in [0.6, 0.7, 0.75, 0.8, 0.85]:
        sd = pd.to_datetime(Xy['date'].quantile(q))
        train = Xy[Xy['date'] < sd]
        test  = Xy[Xy['date'] >= sd]
        if has_two_classes(train['hotspot_next']) and has_two_classes(test['hotspot_next']):
            print(f"[INFO] Adjusted split_date to {sd.date()} (q={q}) to get both classes.")
            return train, test, sd

    return None, None, None

train, test, used_split = make_split_with_two_classes(Xy, split_date)

if train is None:
    # Final fallback: build a percentile label so we actually have two classes,
    # then try the quantile split again.
    print("[WARN] Could not find a split with two classes. Falling back to percentile label (bottom 20%).")
    p20 = df['ndvi_z_next'].quantile(0.20)
    df['hotspot_next'] = (df['ndvi_z_next'] <= p20).astype('int8')
    Xy = df.dropna(subset=feat_cols + ['hotspot_next']).copy()
    train, test, used_split = make_split_with_two_classes(Xy, Xy['date'].quantile(0.8))

print("Class counts (overall):")
print(Xy['hotspot_next'].value_counts())
print("Train counts:", train['hotspot_next'].value_counts().to_dict())
print("Test counts :", test['hotspot_next'].value_counts().to_dict())
print("Split date  :", used_split)

# ---- train / evaluate
X_train = train[feat_cols].values
y_train = train['hotspot_next'].values
X_test  = test[feat_cols].values
y_test  = test['hotspot_next'].values

clf = LogisticRegression(max_iter=300, class_weight='balanced')
clf.fit(X_train, y_train)
proba = clf.predict_proba(X_test)[:, 1]

roc = roc_auc_score(y_test, proba)
pr  = average_precision_score(y_test, proba)
p10 = precision_at_k(y_test, proba, 0.10)
print(f"Logistic Regression ROC-AUC={roc:.3f}  PR-AUC={pr:.3f}  Precision@10%={p10:.3f}")

"""Step 5:Score and Export"""

import json
import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path

# ---- choose the model that exists
model = None
if 'xgb' in globals():
    model = xgb
else:
    model = clf

# ---- pick a scoring date: last date before the split you actually used
# (If you followed the robust split code I gave, you should have `used_split`.)
if 'used_split' in globals() and pd.notnull(used_split):
    score_date = df[df['date'] < used_split]['date'].max()
else:
    # fallback: score on penultimate date overall
    score_date = df['date'].sort_values().iloc[-2]
print("Scoring date (t):", score_date)

# ---- reconstruct/ensure lag features exist (safe if you already did this)
for col in ['ndvi','rain_16d','ndvi_z']:
    if f'{col}_lag1' not in df.columns:
        df[f'{col}_lag1'] = df.groupby('cell_id')[col].shift(1)
    if f'{col}_lag2' not in df.columns:
        df[f'{col}_lag2'] = df.groupby('cell_id')[col].shift(2)

# ---- take all rows at score_date with complete feature vector
score = df[df['date'] == score_date].copy()
score = score.dropna(subset=feat_cols).copy()
print("Rows to score (cells at t with full features):", len(score))

# ---- predict probability of hotspot at next period
score['p_hotspot_next'] = model.predict_proba(score[feat_cols].values)[:, 1]
score_out = score[['cell_id','date','p_hotspot_next']].reset_index(drop=True)
print(score_out.head())

"""Step 6:Join with grid points & Save files"""

COUNTY = "Wajir"   # change if youâ€™re running this notebook for another county
PREFIX = "wajir_hotspot_forecast"

# 1) join with grid points GeoJSON for map
points_gdf = gpd.read_file(points_geo)
if points_gdf.crs is None:
    points_gdf.set_crs(4326, inplace=True)   # assume WGS84 if missing
else:
    points_gdf = points_gdf.to_crs(4326)

mapped = points_gdf.merge(score_out, on='cell_id', how='left').dropna(subset=['p_hotspot_next'])

# 2) output paths
OUT_DIR = Path('/content') if 'OUT_DIR' not in globals() else OUT_DIR
geojson_out = OUT_DIR / f'{PREFIX}.geojson'
csv_out     = OUT_DIR / f'{PREFIX}.csv'
metrics_out = OUT_DIR / f'{COUNTY.lower()}_model_metrics.txt'

# 3) save GeoJSON & CSV for Streamlit
mapped.to_file(geojson_out, driver='GeoJSON')
mapped[['cell_id','p_hotspot_next']].sort_values('p_hotspot_next', ascending=False).to_csv(csv_out, index=False)

# 4) save simple metrics file (handles both clf/xgb cases)
lines = []
lines.append(f"County: {COUNTY}")
lines.append(f"Scored period (t): {pd.to_datetime(score_date).date()}")
try:
    # if you computed these earlier, reuse; otherwise compute minimal ones now
    from sklearn.metrics import roc_auc_score, average_precision_score, precision_score
    if 'y_test' in globals() and 'proba' in globals():
        lines.append(f"ROC-AUC: {roc_auc_score(y_test, proba):.4f}")
        lines.append(f"PR-AUC : {average_precision_score(y_test, proba):.4f}")
    elif 'y_test' in globals() and 'proba_xgb' in globals():
        lines.append(f"ROC-AUC: {roc_auc_score(y_test, proba_xgb):.4f}")
        lines.append(f"PR-AUC : {average_precision_score(y_test, proba_xgb):.4f}")
except Exception as e:
    lines.append(f"Metrics note: {e}")

with open(metrics_out, 'w') as f:
    f.write("\n".join(lines))

print("Saved files:")
print(" -", geojson_out)
print(" -", csv_out)
print(" -", metrics_out)