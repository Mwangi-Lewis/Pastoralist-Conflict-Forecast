# -*- coding: utf-8 -*-
"""Turkana_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlTpEzReh0BH_3CPKjcqdP2DDiqa9YDP

Step 1:Import the necessary libraries
"""

!pip -q install geopandas pyogrio shapely pydeck folium contextily xgboost

import pandas as pd
import numpy as np
import geopandas as gpd
from pathlib import Path

# If using Google Drive, uncomment:
from google.colab import drive
drive.mount('/content/drive')


ndvi_csv   = "/content/drive/MyDrive/turkana_ndvi_pointgrid_2019_2022.csv"
rain_csv   = "/content/drive/MyDrive/turkana_chirps16d_2019_2022.csv"
points_geo = "/content/drive/MyDrive/turkana_grid_points_5km.geojson"

ndvi_path = Path(ndvi_csv)
rain_path = Path(rain_csv)
points_path = Path(points_geo)

"""Step 2:Loading the dataset"""

def load_ndvi(p: Path) -> pd.DataFrame:
    df = pd.read_csv(p)
    # Normalize expected columns
    df = df.rename(columns={'ndvi_mean': 'ndvi'})
    df['date'] = pd.to_datetime(df['date'])
    return df[['cell_id', 'date', 'ndvi']]

def load_rain(p: Path) -> pd.DataFrame:
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'])
    return df[['cell_id', 'date', 'rain_16d']]

ndvi = load_ndvi(ndvi_path)
rain = load_rain(rain_path)

df = (ndvi.merge(rain, on=['cell_id','date'], how='inner')
            .sort_values(['cell_id','date'])
            .reset_index(drop=True))

print("Merged shape:", df.shape)
print(df.head())

# -----------------------------------------
# 2) NDVI anomaly (z-score) by 'fortnight'
#    climatology per cell & fortnight index
# -----------------------------------------
# Fortnight index: 0..22/23
df['fortnight'] = ((df['date'].dt.dayofyear - 1) // 16).astype(int)

# Compute per-cell, per-fortnight climatology
grp = df.groupby(['cell_id','fortnight'])
mu = grp['ndvi'].transform('mean')
sd = grp['ndvi'].transform('std').replace(0, np.nan)

df['ndvi_z'] = (df['ndvi'] - mu) / sd

# ------------------------------------------------
# 3) Create lag features and next-period target
# ------------------------------------------------
def add_lags(g: pd.DataFrame, cols=('ndvi','rain_16d','ndvi_z'), L=(1,2)):
    for c in cols:
        for k in L:
            g[f'{c}_lag{k}'] = g[c].shift(k)
    g['ndvi_z_next'] = g['ndvi_z'].shift(-1)
    return g

df = df.groupby('cell_id', group_keys=False).apply(add_lags)

# Classification target: next period stressed? (z < -1)
df['hotspot_next'] = (df['ndvi_z_next'] < -1.0).astype('int8')

# Feature set
feat_cols = [
    'ndvi','rain_16d','ndvi_z',
    'ndvi_lag1','ndvi_lag2',
    'rain_16d_lag1','rain_16d_lag2',
    'ndvi_z_lag1','ndvi_z_lag2'
]

Xy = df.dropna(subset=feat_cols + ['hotspot_next']).copy()
print("Rows with complete features/label:", Xy.shape[0])
print("Class balance (1=hotspot):")
print(Xy['hotspot_next'].value_counts(normalize=True).rename('ratio'))

# ------------------------------------------
# 4) Time-based split: last year as test set
# ------------------------------------------
split_date = pd.Timestamp('2022-01-01')
train = Xy[Xy['date'] < split_date]
test  = Xy[Xy['date'] >= split_date]

X_train = train[feat_cols].to_numpy()
y_train = train['hotspot_next'].to_numpy()
X_test  = test[feat_cols].to_numpy()
y_test  = test['hotspot_next'].to_numpy()

# --------------------------
# 5) Train baseline (LogReg)
# --------------------------
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, precision_score

logreg = LogisticRegression(max_iter=300, class_weight='balanced', n_jobs=None)
logreg.fit(X_train, y_train)
proba_lr = logreg.predict_proba(X_test)[:,1]

roc_lr = roc_auc_score(y_test, proba_lr)
pr_lr  = average_precision_score(y_test, proba_lr)

# Precision@K helper (e.g., top 10% most risky cells)
def precision_at_k(y_true, y_score, k=0.10):
    n = max(1, int(len(y_score) * k))
    idx = np.argsort(-y_score)[:n]
    preds = np.zeros_like(y_true)
    preds[idx] = 1
    return precision_score(y_true, preds, zero_division=0)

p_at_10 = precision_at_k(y_test, proba_lr, k=0.10)

print(f"[LogReg] ROC-AUC={roc_lr:.3f} | PR-AUC={pr_lr:.3f} | Precision@10%={p_at_10:.3f}")
print(classification_report(y_test, (proba_lr >= 0.5).astype(int), digits=3))

try:
    from xgboost import XGBClassifier
    xgb = XGBClassifier(
        n_estimators=400, max_depth=4, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
        eval_metric='auc', tree_method='hist'
    )
    xgb.fit(X_train, y_train)
    proba_xgb = xgb.predict_proba(X_test)[:,1]
    roc_xgb = roc_auc_score(y_test, proba_xgb)
    pr_xgb  = average_precision_score(y_test, proba_xgb)
    p10_xgb = precision_at_k(y_test, proba_xgb, k=0.10)
    print(f"[XGB]    ROC-AUC={roc_xgb:.3f} | PR-AUC={pr_xgb:.3f} | Precision@10%={p10_xgb:.3f}")
    use_model = ('xgb', xgb, proba_xgb)
except Exception as e:
    print("XGBoost not available / failed, sticking to Logistic Regression.", e)
    use_model = ('logreg', logreg, proba_lr)

model_name, model_obj, _ = use_model

# ------------------------------------------------------
# 6) Create a “forecast” layer for the last train period
# ------------------------------------------------------
# Pick the final date BEFORE split (simulate forecasting the next period)
score_date = df[df['date'] < split_date]['date'].max()
# We already computed lags in df; just select that date’s rows with complete features
score_rows = df[(df['date'] == score_date)].copy()
score_rows = score_rows.dropna(subset=feat_cols).copy()

score_rows['p_hotspot_next'] = model_obj.predict_proba(score_rows[feat_cols].to_numpy())[:,1]
print("Scored rows for map:", len(score_rows), "date:", score_date.date())

# ------------------------------------------------
# 7) Join with 5-km grid points and export files
# ------------------------------------------------
gdf = gpd.read_file(points_path).to_crs(4326)
mapped = gdf.merge(score_rows[['cell_id','p_hotspot_next']], on='cell_id', how='left')
mapped = mapped.dropna(subset=['p_hotspot_next'])

out_dir = Path('/content')  # change if you prefer your Drive folder
geojson_out = out_dir / 'turkana_hotspot_forecast.geojson'
csv_out     = out_dir / 'turkana_hotspot_forecast.csv'
metrics_out = out_dir / 'model_metrics.txt'

mapped[['cell_id','p_hotspot_next','geometry']].to_file(geojson_out, driver='GeoJSON')
mapped[['cell_id','p_hotspot_next']].sort_values('p_hotspot_next', ascending=False).to_csv(csv_out, index=False)

with open(metrics_out, 'w') as f:
    f.write(f"Model: {model_name}\n")
    f.write(f"ROC-AUC: {roc_lr:.4f}\n" if model_name=='logreg' else f"ROC-AUC: {roc_xgb:.4f}\n")
    f.write(f"PR-AUC : {pr_lr:.4f}\n" if model_name=='logreg' else f"PR-AUC : {pr_xgb:.4f}\n")
    f.write(f"Precision@10%: {p_at_10:.4f}\n" if model_name=='logreg' else f"Precision@10%: {p10_xgb:.4f}\n")
    f.write(f"Scored period (t): {score_date.date()}\n")
print("Saved files:")
print(" -", geojson_out)
print(" -", csv_out)
print(" -", metrics_out)